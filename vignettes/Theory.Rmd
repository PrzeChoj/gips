---
title: "Theory"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Theory}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk[["set"]](
  collapse = TRUE,
  comment = "#>",
  cache = TRUE
)
```

# Theory the `gips` is based on

The package is based on the [paper [1] from references](https://arxiv.org/abs/2004.03503.pdf). The math behind the package is precisely demonstrated, and all the theorems are proven.

In this vignette, we would like to give a gentle introduction. We want to point out all the most important results from this work from the user's point of view. We will also show examples of those results in the `gips` package.

```{r setup}
library(gips)
```

As we read in the abstract, the outline of the paper is to "derive the distribution of the maximum likelihood estimate of the covariance parameter $\Sigma$ (...)" and then to "perform Bayesian model selection in the class of complete Gaussian models invariant by the action of a subgroup of the symmetric group (...)". Those ideas are implemented in the `gips` package.


# Basic definitions

Let $V=\{1,\ldots,p\}$ be a finite index set, and $Z=(Z_1,\ldots, Z_p)^\top$ be a multivariate random variable following a centered Gaussian model $\mathrm{N}_p(0,\Sigma)$.
Let $\mathfrak{S}_p$ denote the symmetric group
on $V$, that is, the set of all permutations on $\{1,\ldots,p\}$ with function composition as the group operation. Let $\Gamma$ be an arbitrary subgroup of $\mathfrak{S}_p$. The model $\mathrm{N}_p(0,\Sigma)$ is said to be invariant under the action of $\Gamma$ if for all $g\in \Gamma$, $g\cdot\Sigma\cdot g^\top=\Sigma$ 
(here, we identify a permutation $g$ with its permutation matrix).

For a subgroup $\Gamma \subset  \mathfrak{S}_p$, we define the colored space, i.e. the space of symmetric matrices invariant under $\Gamma$, $$\mathcal{Z}_{\Gamma} := \{S \in \mathrm{Sym}(p;\mathbb{R})\colon S_{ij} = S_{\sigma(i)\sigma(j)} \text{ for all }\sigma \in \Gamma\mbox{ for all }i,j\in V\},$$ and the colored cone of positive definite matrices valued in $\mathcal{Z}_{\Gamma}$, $$\mathcal{P}_{\Gamma} := \mathcal{Z}_{\Gamma} \cap \mathrm{Sym}^+(p;\mathbb{R}).$$

# Block Decomposition - [1, Theorem 1]

The main theoretical result in this theory (Theorem 1 in [1]) states that given a permutation subgroup $\Gamma$ there exist structure constants $(k_i,d_i,r_i)_{i=1}^L$ and orthogonal matrix $U_\Gamma$ such that all the symmetric matrices $S\in\mathcal{Z}_\Gamma$ can be transformed into block-diagonal form.

It is worth pointing out that constants $k = d$ for cyclic group $\Gamma = \left<\sigma\right>$ and that `gips` searches within cyclic subgroups only. Consider example:

```{r th1_1}
p <- 6
S <- matrix(c(1.1,0.9,0.8,0.7,0.8,0.9,
              0.9,1.1,0.9,0.8,0.7,0.8,
              0.8,0.9,1.1,0.9,0.8,0.7,
              0.7,0.8,0.9,1.1,0.9,0.8,
              0.8,0.7,0.8,0.9,1.1,0.9,
              0.9,0.8,0.7,0.8,0.9,1.1), nrow = p)
```
`S` is a symmetric matrix invariant under the group $\Gamma = \left<(1,2,3,4,5,6)\right>$.

```{r th1_2, dependson="th1_1"}
g_perm <- gips_perm("(1,2,3,4,5,6)", p)
U_Gamma <- prepare_orthogonal_matrix(g_perm)

block_decomposition <- t(U_Gamma) %*% S %*% U_Gamma
round(block_decomposition, 5)
```
The transformed matrix is in the block-diagonal form of [1,Theorem 1]. The result was rounded to the 5th place after the decimal to hide the inaccuracies of floating point arithmetic.

Let's see the other example:
```{r th1_3, dependson="th1_2"}
p <- 6
S <- matrix(c(1.2,0.9,0.9,0.4,0.2,0.1,
              0.9,1.2,0.9,0.1,0.4,0.2,
              0.9,0.9,1.2,0.2,0.1,0.4,
              0.4,0.1,0.2,1.2,0.9,0.9,
              0.2,0.4,0.1,0.9,1.2,0.9,
              0.1,0.2,0.4,0.9,0.9,1.2), nrow = p)
```
Now, `S` is a symmetric matrix invariant under the group $\Gamma = \left<(1,2,3)(4,5,6)\right>$.

```{r th1_4, dependson="th1_3"}
g_perm <- gips_perm("(1,2,3)(4,5,6)", p)
U_Gamma <- prepare_orthogonal_matrix(g_perm)

block_decomposition <- t(U_Gamma) %*% S %*% U_Gamma
round(block_decomposition, 5)
```
Again, this result is in accordance with [1,Theorem 1]. Note the zeros: $\forall_{i\in\{1,2\},j\in\{3,4,5,6\}}\text{block_decomposition}[i,j] = 0$

# Project Matrix - [1,Eq. (6)]

One can also take any symmetric matrix `S` and find the orthogonal projection on $\mathcal{Z}_{\Gamma}$, the space of matrices invariant under the given permutation:

$$\pi_\Gamma(S) := \frac{1}{|\Gamma|}\sum_{\sigma\in\Gamma}\sigma\cdot S\cdot\sigma^\top\in\mathcal{Z}_{\Gamma}$$

```{r def3_1}
p <- 6
n <- 10
Z <- matrix(runif(n*p, min = -10, max = 10), ncol = p)
S <- t(Z) %*% Z / n
round(S, 2)
```

```{r def3_2, dependson="def3_1", echo=FALSE}
plot(gips(S, n), type="heatmap")
```

```{r def3_3, dependson="def3_2"}
S_projected <- project_matrix(S, perm = "(1,2)(3,4,5,6)")
round(S_projected, 2)
```

```{r def3_4, dependson="def3_3", echo=FALSE}
plot(gips(S_projected, n), type="heatmap")
```

TODO(Point out the equal elements of those matrices)

## $C_\sigma$ and `n0`
It is a well-known fact that without additional assumptions, the Maximum Likelihood Estimator (MLE) of the covariance matrix exists if and only if $n \ge p$. However, if the additional assumption is added as the covariance matrix is invariant under permutation $\sigma$, then the sample size $n$
required for the MLE to exist is lower than $p$. It is equal to the number of cycles, denoted hereafter by $C_\sigma$.

For example, if the permutation $\sigma = (1,2,3,4,5,6)$ is discovered by the `find_MAP()` function, then there is a single cycle in it $C_\sigma = 1$. Therefore a single observation would be enough to estimate a covariance matrix with `project_matrix()`. If the permutation $\sigma = (1,2)(3,4,5,6)$ is discovered, then $C_\sigma = 2$ and so 2 observations would be enough.

To get this $C_\sigma$ number in `gips`, one can call `summary()` on the appropriate `gips` object:
```{r n0_2, dependson="def3"}
g1 <- gips(S, n, perm = "(1,2,3,4,5,6)", was_mean_estimated = FALSE)
summary(g1)$n0
g2 <- gips(S, n, perm = "(1,2)(3,4,5,6)", was_mean_estimated = FALSE)
summary(g2)$n0
```
This is called `n0` and not $C_\sigma$, because it is increased by 1 when the mean was estimated:

```{r n0_1, dependson="def3"}
S <- cov(Z)
g1 <- gips(S, n, perm = "(1,2,3,4,5,6)", was_mean_estimated = TRUE)
summary(g1)$n0
g2 <- gips(S, n, perm = "(1,2)(3,4,5,6)", was_mean_estimated = TRUE)
summary(g2)$n0
```

# A Posteriori

When one has the data matrix `Z`, one would like to know if it has a hidden structure of dependencies between features. Luckily, the paper demonstrates a way how to find it.

`gips` performs Bayesian model selection. The method has two parameters of the prior distribution, `delta` and `D_matrix`. By default they are set to 3 and `diag(p)`, respectively. However, it is worth to run the procedure for several parameters `D_matrix` of the form $D\cdot diag(p)$ for positive constant $D$. Large $D$ (compared to the data) favors small groups. 

Namely, the formula [[1,(30)]]((https://arxiv.org/abs/2004.03503.pdf)) states the proportion of A Posteriori distribution given data. With that, one can easily decide which permutation is more likely given the data.

In other words, the formula (30) states how likely the provided data was drawn from the invariant distribution under a given permutation (accurate to within a constant). When one calculates this likelihood on all permutations, one will know which permutation is the most likely. It may be the trivial $\text{id} = (1)(2)...(p)$ permutation or the other one.

Let's see the example:

<div id="spoiler3" style="display:none">
```{r example2_readme1}
# Prepare model, multivariate normal distribution
perm_size <- 6
number_of_observations <- 4
mu <- numeric(perm_size)  
sigma_matrix <- matrix(
  data = c(
    1.05, 0.8, 0.6, 0.4, 0.6, 0.8,
    0.8, 1.05, 0.8, 0.6, 0.4, 0.6,
    0.6, 0.8, 1.05, 0.8, 0.6, 0.4,
    0.4, 0.6, 0.8, 1.05, 0.8, 0.6,
    0.6, 0.4, 0.6, 0.8, 1.05, 0.8,
    0.8, 0.6, 0.4, 0.6, 0.8, 1.05
  ),
  nrow = perm_size, byrow = TRUE
) # sigma_matrix is a matrix invariant under permutation (1,2,3,4,5,6)

# Generate example data from a model:
withr::with_seed(1234,{
  Z <- MASS::mvrnorm(number_of_observations, mu = mu, Sigma = sigma_matrix)
})
# End of prepare model
```
</div>

<button title="Click to show the data preparation" type="button"
   onclick="if(document.getElementById('spoiler3') .style.display=='none')
              {document.getElementById('spoiler3') .style.display=''}
            else{document.getElementById('spoiler3') .style.display='none'}">
  Show/hide data preparation
</button>

Let's say we have this data, `Z`. It has only $4$ observations, and its dimension is $p=6$.
Let's assume `Z` was drawn from the normal distribution with the mean $(0,0,0,0,0,0)$. We want to estimate the covariance matrix:

```{r, example2_readme2, dependson="example2_readme1"}
dim(Z)
number_of_observations <- nrow(Z) # 4
perm_size <- ncol(Z) # 6

# Calculate the covariance matrix from the data (assume the mean is 0):
S <- (t(Z) %*% Z) / number_of_observations

# Make the gips object out of data:
g <- gips(S, number_of_observations, was_mean_estimated = FALSE)

g_map <- find_MAP(g, optimizer = "full")
print(g_map)

S_projected <- project_matrix(S, g_map[[1]])
```

We see the formula [1,(30)] gave the biggest value for the permutation $(1,2,3,4,5,6)$. It was over 150 times bigger than for the trivial $\text{id} = (1)(2)...(p)$ permutation. We interpret that under the assumptions (normality and the mean 0), it is over 150 times more reasonable to assume the data `Z` was drawn from model $\mathrm{N}_p(0,\text{S_projected})$ than from model $\mathrm{N}_p(0,\text{S})$.


# Finding the MAP Estimator

The space of all permutations is enormous for bigger $p$. This is why the authors of the [[1]](https://arxiv.org/abs/2004.03503.pdf) suggested analyzing it with the Metropolis-Hastings algorithm instead of browsing the whole of it.

For more information, see `vignette("Optimizers")` or its [pkgdown page](https://przechoj.github.io/gips/articles/Optimizers.html).


# References

[1] Piotr Graczyk, Hideyuki Ishi, Bartosz Kolodziejek, Hélène Massam. "Model selection in the space of Gaussian models invariant by symmetry." The Annals of Statistics, 50(3) 1747-1774 June 2022. [arXiv link](https://arxiv.org/abs/2004.03503.pdf); [DOI: 10.1214/22-AOS2174](https://doi.org/10.1214/22-AOS2174)
