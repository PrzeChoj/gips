---
title: "Optimizers"
author: "Przemysław Chojecki, Paweł Morgen, Bartosz Kołodziejek"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Optimizers}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = TRUE
)

options(scipen=999) # turn off scientific notation
```

# Available optimizers

To find the Maximum A Posteriori Estimation, one wants to call the `find_MAP()` function. This is designed to find a permutation $\sigma$ so that the value of A Posteriori $f(\sigma)$ is as big as possible. However, the space of permutations is enormous - for permutation of size $p$, the space of all permutations is of size $p!$ ($p$ factorial). Even for $p=19$, this space is practically impossible to browse. This is why `find_MAP()` implements multiple optimizers to choose from:

* `"Metropolis_Hastings"`, `"MH"`
* `"hill_climbing"`, `"HC"`
* `"brute_force"`, `"BF"`, `"full"`

```{r setup}
library(gips)
```

```{r help, cache = TRUE, eval=FALSE}
?find_MAP
```


# Metropolis Hastings

This optimizer is implementation of the [*Second approach* from references(1), section 4.1.2](https://arxiv.org/abs/2004.03503).

This uses the Metropolis-Hastings algorithm to optimize the space; [see Wikipedia](https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm). This algorithm used in this context is a special case of the Simulated Annealing the reader may be more familiar with; [see Wikipedia](https://en.wikipedia.org/wiki/Simulated_annealing).

### Short description

In every iteration $i$, an algorithm is in a permutation; call it $\sigma_i$. Then the random transposition is drawn uniformly $t_i = (j,k)$ and value of $f(\sigma_i \circ t_i)$ is computed.

* If new value is bigger ($f(\sigma_i \circ t_i) \ge f(\sigma_i)$), then $\sigma_{i+1} = \sigma_i \circ t_i$ and we know the new $\sigma$ is better than the old one. 
* If new value is smaller ($f(\sigma_i \circ t_i) < f(\sigma_i)$), then we will choose $\sigma_{i+1} = \sigma_i \circ t_i$ with probability $\frac{f(\sigma_i \circ t_i)}{f(\sigma_i)}$, or we will choose $\sigma_{i+1} = \sigma_i$ with complementary probability $1 - \frac{f(\sigma_i \circ t_i)}{f(\sigma_i)}$.

The final value is the best $\sigma$ that was ever computed.

### Notes

This algorithm was tested in multiple settings and turned out to be an outstanding optimizer. Especially given it does not need any hyperparameters tuned.

The only parameter it depends on is `max_iter`, which determines the number of steps described above. One should choose this number rationally. When decided too small, there is a missed opportunity to find potentially a much better permutation. When decided too big, there is a lost time and computational power that does not lead to growth. Our recommendation is to plot the convergence plot with logarithmic OX scale: `plot(g_map, type = "both", logarithmic_x = TRUE)`. Then decide if the line has flattened already. Keep in mind that the OY scale is also logarithmic. For example, a small change on the OY scale could mean $10000$ **times** the change in A Posteriori.

This algorithm has been analyzed extensively by theoretical probabilists. This lead to interesting results like, for example, that the frequency of visits can estimate the probability of the state. This is the approach explained in the [references(1), section 4.1.2](https://arxiv.org/abs/2004.03503) and shown in [references(1), section 5.2](https://arxiv.org/abs/2004.03503). One can do this by setting `return_probabilities = TRUE`.

### Example

```{r Metropolis_Hastings, cache=TRUE}
perm_size <- 70
mu <- runif(perm_size, -10, 10) # Assume we don't know the mean
sigma_matrix <- (function(x){t(x) %*% x})(matrix(rnorm(perm_size*perm_size), nrow=perm_size))
number_of_observations <- 50
Z <- MASS::mvrnorm(number_of_observations, mu = mu, Sigma = sigma_matrix)
S <- cov(Z) # Assume we have to estimate the mean

g <- gips(S, number_of_observations)
plot(g, type = 'heatmap')

# TODO(Make the max_iter bigger:)
g_map <- find_MAP(g, max_iter = 10, optimizer = "Metropolis_Hastings")
g_map

plot(g_map, type = 'both')
```


# Hill climbing

This optimizer walks on the space of permutations. Let's say it is in permutation $\sigma$. Then, in iteration it will calculate all posteriori probabilities of permutations of a form $\sigma \circ (i,j)$, where $1 \le i < j \le \text{perm_size}$. Then it will choose the best among those and start the next iteration from the new $\sigma$.

### Mathy definition:

$$\sigma_0 = ()$$

$$\sigma_{i+1} = argmax_{\text{perm} \in \text{neighbors}(\sigma_{i})}\{\text{posteriori}(perm)\}$$

Where:
$$\text{neighbors}(\sigma) = \{\sigma \circ (i,j) : 1 \le i < j \le \text{perm_size}\}$$

### Pseudocode:

```{r, hill_climb_pseudocode, eval=FALSE}
hill_climb <- function(g){
  perm <- g[[1]]
  perm_posteriori <- log_posteriori_of_gips(g)
  perm_size <- attr(perm, "size")
  S <- attr(g, "S")
  number_of_observations <- attr(g, "number_of_observations")
  
  best_neighbour <- NULL
  best_neighbour_posteriori <- - Inf
  
  while (best_neighbour_posteriori > perm_posteriori){
    best_neighbour <- NULL
    best_neighbour_posteriori <- - Inf
    
    for (i in 1:(perm_size-1)){
      for (j in (i+1):perm_size){
        neighbour <- gips:::compose_with_transposition(perm, c(i, j))
        neighbour_posteriori <- log_posteriori_of_gips(gips(S, number_of_observations,
                                                            perm = neighbour))
        
        if (neighbour_posteriori > best_neighbour_posteriori){
          best_neighbour <- neighbour
          best_neighbour_posteriori <- neighbour_posteriori
        } # end if
      } # end for j
    } # end for i
  } # end while
  
  return(best_neighbour)
}
```

### Example:

```{r hill_climbing, cache=TRUE}
perm_size <- 25
mu <- runif(perm_size, -10, 10) # Assume we don't know the mean
sigma_matrix <- (function(x){t(x) %*% x})(matrix(rnorm(perm_size*perm_size), nrow=perm_size))
number_of_observations <- 20
Z <- MASS::mvrnorm(number_of_observations, mu = mu, Sigma = sigma_matrix)
S <- cov(Z) # Assume we have to estimate the mean

g <- gips(S, number_of_observations)
plot(g, type = 'heatmap')

# TODO(Make the max_iter bigger:)
g_map <- find_MAP(g, max_iter = 2, optimizer = "hill_climbing")
g_map
plot(g_map, type = 'both')
```


# Brute Force

This is the only optimizer that will certainly find the actual MAP Estimator. This is only recommended for small spaces ($p \le 8$). It can also browse bigger spaces, but the required time is probably too long.

### Example:

```{r brute_force, cache=TRUE}
perm_size <- 6
mu <- runif(perm_size, -10, 10) # Assume we don't know the mean
sigma_matrix <- matrix(
  data = c(
    1.0, 0.8, 0.6, 0.4, 0.6, 0.8,
    0.8, 1.0, 0.8, 0.6, 0.4, 0.6,
    0.6, 0.8, 1.0, 0.8, 0.6, 0.4,
    0.4, 0.6, 0.8, 1.0, 0.8, 0.6,
    0.6, 0.4, 0.6, 0.8, 1.0, 0.8,
    0.8, 0.6, 0.4, 0.6, 0.8, 1.0
  ),
  nrow = perm_size, byrow = TRUE
) # sigma_matrix is a matrix invariant under permutation (1,2,3,4,5,6)
number_of_observations <- 13
Z <- MASS::mvrnorm(number_of_observations, mu = mu, Sigma = sigma_matrix)
S <- cov(Z) # Assume we have to estimate the mean

g <- gips(S, number_of_observations)

g_map <- find_MAP(g, optimizer = "brute_force")
g_map
```

# Additional parameters

The `find_MAP()` function has also have 2 additional parameters, namely `show_progress_bar` and `save_all_perms`. Both can be set to `TRUE` or `FALSE`.

The `show_progress_bar = TRUE` simply means `gips` will print the progress on the console. Keep in mind that when one sets the `return_probabilities = TRUE`, there will be a second progress bar indicating calculating the probabilities after optimization.

The `save_all_perms = TRUE` means that `gips` will save all visited permutations in the outputted object. This will significantly increase the RAM needed for this object (for example, for $p=150$ and `max_perm = 150000`, we needed 400 MB to store it, while with `save_all_perms = FALSE`, the 2 MB was enough). However, this is necessary for `return_probabilities = TRUE` or more complex path analysis.


# Discussion

We are aware that the optimizers mentioned above try to optimize the space of **permutations**. This is not precisely the thing we want because the found permutation is interpreted as the generator or the group of permutations. We tested the algorithms working correctly in this setting, and we are yet to see any practical advantage of optimization performed on groups. However, the theoretical advantage is clear, as the results are easier to interpret.

We encourage everyone to leave a comment on available and potential new optimizers on the [ISSUE#21](https://github.com/PrzeChoj/gips/issues/21). There, one can also find the implemented optimizers but not yet added to `gips`.


# References

(1) Piotr Graczyk, Hideyuki Ishi, Bartosz Kolodziejek, Hélène Massam. "Model selection in the space of Gaussian models invariant by symmetry." The Annals of Statistics, 50(3) 1747-1774 June 2022. [arXiv link](https://arxiv.org/abs/2004.03503.pdf); [DOI: 10.1214/22-AOS2174](https://doi.org/10.1214/22-AOS2174)
