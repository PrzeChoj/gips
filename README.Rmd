---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk[['set']](
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  cache = TRUE
)
```

# `gips` <a href="https://przechoj.github.io/gips/"><img src="man/figures/logo.svg" align="right" height="139" /></a>

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![CRAN status](https://www.r-pkg.org/badges/version/gips)](https://CRAN.R-project.org/package=gips)
[![R-CMD-check](https://github.com/PrzeChoj/gips/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/PrzeChoj/gips/actions/workflows/R-CMD-check.yaml)
[![test-coverage](https://codecov.io/gh/PrzeChoj/gips/branch/main/graph/badge.svg)](https://codecov.io/gh/PrzeChoj/gips?branch=main)
<!-- badges: end -->

gips - Gaussian model Invariant by Permutation Symmetry

`gips` is an R package that looks for permutation symmetries in the Gaussian sample. Such symmetries reduce the free parameters in the unknown covariance matrix. This is especially useful when the number of variables is substantially larger than the number of observations.


## `gips` will help you with two things:
1. Finding hidden symmetries between the variables. `gips` can be used as an exploratory tool for searching the space of permutation symmetries of the Gaussian vector.  Useful in the Exploratory Data Analysis (EDA).
2. Covariance estimation. The MLE for the covariance matrix is known to exist if and only if the number of variables is less or equal to the number of observations. Additional knowledge of symmetries significantly weakens this requirement. Moreover, the reduction of model dimension brings the advantage in terms of precision of covariance estimation. 

## Installation

From [GitHub](https://github.com/PrzeChoj/gips):
``` r
# Install the development version from GitHub:
# install.packages("devtools")
devtools::install_github("PrzeChoj/gips")
```

From [CRAN](https://CRAN.R-project.org/package=gips) (it is yet to be published):
``` r
# Install the released version from CRAN:
install.packages("gips")
```

## Examples

### Example 1 - EDA

Assume we have the data, and we want to understand its structure:

```{r example_mean_unknown1}
library(gips)

Z <- DAAG::oddbooks
```

Assume the data is normally distributed. This is a reasonable assumption, because, for example, p-values for Mardiaâ€™s normality tests: `QuantPsyc::mult.norm(Z)$mult.test` are 0.53 and 0.22, which are > 0.05.

```{r example_mean_unknown2, dependson="example_mean_unknown1"}
Z_scaled <- scale(Z)
dim(Z_scaled)
number_of_observations <- nrow(Z) # 12
perm_size <- ncol(Z) # 4

S <- cov(Z_scaled)
S

g <- gips(S, number_of_observations)
plot(g, type = "heatmap")
```

We can see some strong similarities between columns 2 and 3, representing the book's height and breadth. In this matrix. For example, covariance between [1,2] is very similar to [1,3]. And covariance between [2,4] is very similar to [3,4]. Other covariance does not seem so close to each other.

Let's see if the `find_MAP()` will find this relationship:

```{r example_mean_unknown3, dependson="example_mean_unknown2"}
g_MAP <- find_MAP(g, optimizer = "brute_force")

g_MAP
```

`find_MAP` found the relationship (2,3). In its opinion, the covariance [1,2] and [1,3] are so close to each other that it is reasonable to consider them equal. Similarly, covariance [2,4] and [3,4] will be considered equal:

```{r example_mean_unknown4, dependson="example_mean_unknown3"}
S_projected <- project_matrix(S, g_MAP[[1]])
S_projected

plot(g_MAP, type = "heatmap")
```

Remember that `gips` performed those calculations on the `scale`d version of the dataset, so practically, it was performed on the correlation matrix and not the covariance matrix. The analysis is the same, but one has to remember to come back to the original scaling:

```{r example_mean_unknown5, dependson="example_mean_unknown4"}
sqrt_diag <- diag(sqrt(diag(cov(Z))))
estimated_covariance <- sqrt_diag %*% project_matrix(S, g_MAP[[1]]) %*% sqrt_diag
```


### Example 2 - modeling

First, construct data for the example:
```{r example_mean_known1}
# Prepare model, multivariate normal distribution
perm_size <- 6
mu <- numeric(perm_size)  
sigma_matrix <- matrix(
  data = c(
    1.1, 0.8, 0.6, 0.4, 0.6, 0.8,
    0.8, 1.1, 0.8, 0.6, 0.4, 0.6,
    0.6, 0.8, 1.1, 0.8, 0.6, 0.4,
    0.4, 0.6, 0.8, 1.1, 0.8, 0.6,
    0.6, 0.4, 0.6, 0.8, 1.1, 0.8,
    0.8, 0.6, 0.4, 0.6, 0.8, 1.1
  ),
  nrow = perm_size, byrow = TRUE
) # sigma_matrix is a matrix invariant under permutation (1,2,3,4,5,6)


# Generate example data from a model:
Z <- MASS::mvrnorm(4, mu = mu, Sigma = sigma_matrix)
# End of prepare model
```

Suppose we do not know the true covariance matrix $\Sigma$ and we want to estimate it. We cannot use the standard MLE because it does not exists ($4 < 6$, $n < p$).

We will assume it was generated from the normal distribution with the mean $0$.
```{r example_mean_known2, dependson="example_mean_known1"}
library(gips)
dim(Z)
number_of_observations <- nrow(Z) # 4
perm_size <- ncol(Z) # 6

# Calculate the covariance matrix from the data:
S <- (t(Z) %*% Z) / number_of_observations
```

Make the gips object out of data:
```{r example_mean_known3, dependson="example_mean_known2"}
g <- gips(S, number_of_observations, was_mean_estimated = FALSE)
```

Find the Maximum A Posteriori Estimator for the permutation. Space is small ($6! = 720$), so it is reasonable to browse the whole of it:
```{r example_mean_known4, dependson="example_mean_known3"}
g_map <- find_MAP(g, optimizer = "brute_force")
g_map
```

We see that the found permutation is hundreds of times more likely than making no additional assumption. That means the additional assumptions are justified.
```{r example_mean_known5, dependson="example_mean_known4"}
summary(g_map)$n0
summary(g_map)$n0 <= number_of_observations # 1 <= 4
```

What is more, we see the number of observations ($4$) is bigger or equal to $n_0 = 1$, so we can estimate the covariance matrix with the Maximum Likelihood estimator:
```{r example_mean_known6, dependson="example_mean_known5"}
S_projected <- project_matrix(S, g_map[[1]])
S_projected

# Plot the found matrix:
plot(g_map, type = "heatmap")
```

We see `gips` found the data's structure, and we could estimate the covariance matrix with huge accuracy only with a small amount of data.

Note that the rank of the `S` matrix was 4, while the rank of the `S_projected` matrix was 6.


# Further reading

For more examples and introduction, see `vignette("gips")` or its [pkgdown page](https://przechoj.github.io/gips/articles/gips.html).


# Acknowledgment

Project was funded by Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme.

